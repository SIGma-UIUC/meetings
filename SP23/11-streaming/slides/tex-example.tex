% --------------------------------------------------------------------
% This is a simple Beamer document that uses beamerthemesigma.sty
% Reading the comments should help you create a presentation even if
% you've never used Beamer before.
% --------------------------------------------------------------------
% Set our document class to Beamer
\documentclass[aspectratio=169]{beamer}

% Some packages for nice font encodings in the final PDF
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% To insert images
\usepackage{graphicx}

% Useful packages from the AMS
\usepackage{amsmath,amssymb,amsthm}

% Package for code highlighting
\usepackage{minted}
\setminted{linenos=true, breaklines=true, breakanywhere=true, style=default}
\usemintedstyle{monokai}

% Set a title
\title{Streaming Algorithms and the JL Lemma}

% The subtitle is generally where I'd expect you to put the week
% number, thus:
\subtitle{Week 11}

% Whoever worked on the presentation:
\author{Ryan Ziegler}

% A date, if you'd like.
\date{}

% An institute name, if you're so inclined
% \institute{University of Illinois Urbana-Champaign}

% Use the SIGma theme for this Beamer presentation
\usetheme{sigma}
% --------------------------------------------------------------------
% Begin document
\begin{document}

% Beamer calls each slide a "frame", defined within the environment:
% \begin{frame}
%   <frame content here>
% \end{frame}

% This frame is just the title.
\begin{frame}
\titlepage
\end{frame}

% A frame with the table of contents.
% This frame's title is "Outline".
\begin{frame}{Outline}
  \tableofcontents
\end{frame}

\section{Background}

\frame{\sectionpage}

\subsection{Probability}

\frame{\subsectionpage}

\begin{frame}{A Probability Refresher}
\protect\hypertarget{a-probability-refresher}{}
\begin{itemize}
    \item (Discrete) probability distribution: given a set \(S\) assign some
probability \(p_i\) to each element, so that \(\sum p_i = 1\) \pause
    \item A \emph{random variable \(X\) from a distribution \(D\)} is a variable
whose value is randomly chosen according to some probability
distribution \(D\). Often denoted \(X \sim D\). \pause
    \item Expected value: suppose \(S \subseteq \mathbb{R}\), then
\(\mathbb{E}[X] = \sum p_i S_i\). Intuitively, if we picked a bunch of
\(X\) following \(D\), this is the average value we'd see. \pause
    \item Expectation is a linear operator:
\(\mathbb{E}[X+Y] = \mathbb{E}[X] + \mathbb{E}[Y]\) \pause
    \item Variance: \(\text{Var}(X) = \mathbb{E}[X^2] - \mathbb{E}[X]^2\), a low
variance indicates that most of the time, when we pick \(X\) it will be
close to \(\mathbb{E}[X]\) \pause
    \begin{itemize}
        \item Note that for \(c \in \mathbb{R}\),
\(\text{Var}(cX) = c^2\text{Var}(X)\)
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Even More Probability}
\protect\hypertarget{even-more-probability}{}
\begin{itemize}
    \item Normal distribution:
\(\mathcal{N}(\mu, \sigma^2) = \frac{1}{\sigma \sqrt{2\pi}}\exp\left(-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2\right)\) \pause

    \item Normal distribution is \(2\)-stable: for
\(X \sim \mathcal{N}(\mu_1, \sigma^2_1)\) and
\(Y \sim \mathcal{N}(\mu_2, \sigma^2_2)\),
\(X + Y \sim \mathcal{N}(\mu_1+\mu_2, \sigma^2_1 + \sigma_2^2)\) \pause

    \item \(\chi^2(k)\) distribution: Sum of \(k\) \(\mathcal{N}(0,1)\) random
variables, has expected value \(k\) \pause
    \item Bernoulli distribution: If \(X \sim \text{Bernoulli}(p)\), \(X\) is
\(1\) with probability \(p\) and \(0\) with probability $(1-p)$

\end{itemize}

\end{frame}

\begin{frame}{Independence and Inequalities}
\protect\hypertarget{independence-and-inequalities}{}
\begin{itemize}
    \item A set of random variables is \(k\)-wise independent iff for any \(k\)
variables in the set, \(f(x_1,\dots,x_k)=f(x_1)\cdots f(x_k)\) \pause
    \item For \(k\)-wise independent random variables,
\(\mathbb{E}\left[\prod_{i=1}^k X_i\right] = \prod_{i=1}^k \mathbb{E}[x_i]\) \pause
    \begin{itemize}
        \item Important: \(k\)-wise independence implies \((k-1)\)-wise independence \pause
    \end{itemize}
    \item Chebyshev's inequality:
\(\text{P}(|X - \mathbb{E}[X]| \geq k\sigma) \leq \frac{1}{k^2}\) \pause
    \item Chernoff bound: Let \(X\) be sum of \(h\) fully independent Bernoulli
RVs, and \(\delta \geq 1\).
\(\text{P}(X > (1+\delta)\mathbb{E}[X]) \leq e^{-\delta^2 \mu / 3}\)
\end{itemize}
\end{frame}

\subsection{Streaming and Sketching Algorithms}

\frame{\subsectionpage}

\begin{frame}{Intro to Streaming Algorithms}
\protect\hypertarget{intro-to-streaming-algorithms}{}
\begin{itemize}
\tightlist
\item
  Streaming model: your algorithm receives inputs one-by-one, and you
  don't know how many inputs you'll receive. Too many inputs to store
  them all and compute later \pause
\item
  (*) Example: suppose you want to calculate the \(k\) most watched YouTube
  videos today. It takes too much space to store all the YouTube videos
  and associated view counters, so you want an algorithm that does the
  following: upon recieving a YouTube video ID, update some data
  structure and continue without storing anything on disk. At the end of
  the day, this data structure should tell you the \(k\) most viewed
  videos. \pause
\item
  (*) The above is possible to do \emph{exactly} with only \(O(k)\) space,
  but this is rare. Most streaming algorithms will only output
  approximates that are good with some probability
\end{itemize}
\end{frame}

\begin{frame}{A Template for Sketching Algorithms}
\protect\hypertarget{a-template-for-sketching-algorithms}{}
\begin{itemize}
\tightlist
\item
  First, output a random variable \(Z\) such that
  \(\mathbb{E}[Z] = g(\sigma)\) where \(g(\sigma)\) is the function
  we're estimating for the stream \(\sigma\)
\item
  Usually \(Z\) will have high variance, typically
  \(\text{Var}(Z) \leq g(\sigma)\)
\item
  How to reduce variance? Run the streaming algorithm \(h\) times in
  parallel, and let \(Z^* = \frac{1}{h}\sum Z_i\)

\[\text{Var}(Z^*) = \frac{1}{h}\text{Var}(Z_1) \text{ and }\mathbb{E}[Z^*] = \mathbb{E}[Z_1]\]
\tightlist
\item
  (*) By Chebyshev's inequality,
  \[\text{P}\left(|Z^* - g(\sigma)| > \epsilon g(\sigma) \right) \leq \frac{\epsilon^2}{h} \]
\item
  (*) So, pick \(h = \frac{4}{\epsilon^2}\) for constant failure
  probability of \(\frac{1}{4}\)
\end{itemize}
\end{frame}

\begin{frame}{The Median Trick}
\protect\hypertarget{the-median-trick}{}
\begin{itemize}
\tightlist
\item
  Next goal: \(|Z^* - g(\sigma)| > \epsilon g(\sigma)\) with some small
  probability \(\delta\) \pause
\item
  Naive approach: do Chebyshev's again. Requires
  \(O\left(\frac{1}{\delta \epsilon^2} \right)\) parallel copies. We
  want to do better \pause
\item
  Consider parallel copies \(Z^*_1, \dots, Z^*_k\) that each fail with
  probability \(1/4\) \pause
\item
  Our intuition tells us the median of these estimators should be
  ``good" but how good? \pause
\item
  (*) Let \(X_i = 1\) iff the \(i\)th parallel copy fails, so then
  \(X_i \sim \text{Bernoulli}(1/4)\) \pause
\item
  (*) Define \(X = \sum X_i\), so then \(\mathbb{E}[X] = \frac{k}{4}\) \pause
\item
  (*) By Chernoff bound,
  \[\text{P}\left(X \geq (1+1)\frac{k}{4}\right) \leq e^{-k/12}\] \pause
\item
  (*) So, pick \(k = O(\log(1/\delta))\). Only running
  \(O\left(\frac{\log\left(\frac{1}{\delta}\right)}{\epsilon^2}\right)\)
  independent copies of our algorithm!
\end{itemize}
\end{frame}

\section{Streaming $\ell_2$ Estimation}

\frame{\sectionpage}

\begin{frame}{Frequency Moment Estimation}
\protect\hypertarget{frequency-moment-estimation}{}
\begin{itemize}
\tightlist
\item
  Problem: we receive a stream \(\sigma\) of values
  \(e_1,\dots \in \mathbb{Z}\) where \(1 \leq e_i \leq n\) for some
  \(n\) we know apriori \pause
\item
  Define the frequency vector to be \(f(\sigma) = (f_1,\dots,f_n)\)
  where \(f_i\) is the number of times we've seen \(i\) \pause
\item
  Goal: estimate \(||f(\sigma)||_2^2\) with only
  \(O(\text{polylog}(n))\) space \pause
\item
  Recall the definition of \(L_2\) norm:
  \[||f(\sigma)||_2^2 = \sum_{i=1}^n f_i^2\]
\end{itemize}
\end{frame}

\begin{frame}{AMS F2 Estimation}
\protect\hypertarget{ams-f2-estimation}{}
\begin{itemize}
\tightlist
\item
  Intuition: keep a single variable \(Z\) so that we can output \(Z^2\)
  as our estimate of \(||f(\sigma)||_2^2\) \pause
\item
  (*) Idea: create some random variable \(Y_i\) for each index so that
  \(\mathbb{E}[Z^2] = ||f(\sigma)||_2^2\). In particular,
  \(Z = \sum Y_i f_i\)
  \[ \mathbb{E}[Z^2] = \sum f_i^2 Y_i^2 + 2\sum_{i \neq j} f_i f_j Y_i Y_j\]
\item
  (*) We need \(Y_i\) to be pairwise independent and satisfy
  \(\mathbb{E}[Y_iY_j] = 0\) and \(\mathbb{E}[Y_i^2] = 1\) \pause
\item
  (*) Solution: \(Y_i = 1\) with probability \(\frac{1}{2}\) and
  \(Y_i = -1\) with probability \(\frac{1}{2}\) 
\end{itemize}
\end{frame}

\begin{frame}[fragile]{AMS F2 Estimation Continued}
\protect\hypertarget{ams-f2-estimation-continued}{}
\begin{itemize}
\tightlist
\item
  Creating \(O(n)\) random variables takes up too much space!
\item
  Solution: \(O(1)\)-wise independent hash family of functions
  \([n] \to \{-1,1\}\) can be stored in \(O(\text{polylog}(n))\) space \pause
\item
  (*) Replace each \(Y_i\) with \(h(i)\), and the analysis is the exact same \pause
\item
  (*) Similar analysis shows \(\mathbb{E}[Z^4] \leq 2||f(\sigma)||_2^2\), so
  we can apply average and median idea from before \pause
\end{itemize}

\begin{verbatim}
def ams_f2:
    let h be a hash function from hash family H
    let z = 0
    while i is an item from stream
        z = z + h(i)
    output z
\end{verbatim}
\end{frame}

\begin{frame}[fragile]{Extending F2 Estimation}
\protect\hypertarget{extending-f2-estimation}{}
\begin{itemize}
\tightlist
\item
  Note that we never used the fact that \(f_i\) was positive or integral
\item
  Richer model: receive a stream of updates of the form
  \((i, \Delta_i)\) representing a change to the \(i\)th coordinate of
  our vector \pause
\end{itemize}

\begin{verbatim}
def l2_estimate:
    let h be a hash function from hash family H
    let z = 0
    while (i,d) is an item from stream
        z = z + h(i)d
    output z
\end{verbatim}
\end{frame}

\section{From Stream to Matrix}

\frame{\sectionpage}

\begin{frame}{Linear Sketching}
\protect\hypertarget{linear-sketching}{}
\begin{itemize}
\tightlist
\item
  What we just created is a linear sketch: call our algorithm \(C\). We
  can show that \(C(\sigma_1 + \sigma_2) = C(\sigma_1) + C(\sigma_2)\),
  since each iteration we add to \(Z\)
\item
  (*) Geometric interpretation: our algorithm is an
  \(\frac{\log(1/\delta)\log n}{\epsilon^2} \times n\) matrix \(M\) of
  \(\{-1,1\}\) values, each row is a parallel copy of the streaming
  algorithm \pause
\item
  (*) Now we have \(Mx = y\) where \(y\) is a vector whose length is similar
  to that of \(x\) but is in lower dimension \pause
\item
  (*) Next goal: generalize this idea so that we can reduce the dimension of
  a \emph{set} of vectors while preserving pairwise distances \pause
\item
  (*) Useful in real-world applications such as nearest neighbors, ML, etc
\end{itemize}
\end{frame}

\begin{frame}{The JL Lemma}
\protect\hypertarget{the-jl-lemma}{}
\begin{itemize}
\tightlist
\item
  Let \(M\) be an \(k \times n\) matrix where each entry is chosen
  independently from \(\mathcal{N}(0,1)\)
\item
  Claim: for
  \(k = \Omega\left(\frac{\log(1/\delta)}{\epsilon^2}\right)\), we have
  that with probability \(1 - \delta\),
  \(||\frac{1}{\sqrt{k}}Mx||_2 = (1 \pm \epsilon)||x||_2\) for fixed
  \(x \in \mathbb{R}^n\) \pause
\item
  Immediate corollary: Let \(S\) be a set of \(k\) vectors in
  \(\mathbb{R}^n\), we can preserve pairwise distances with high
  probability by picking
  \(k = \Omega\left(\frac{\log n}{\epsilon^2}\right)\) \pause
\end{itemize}
\end{frame}

\begin{frame}{JL Lemma: Idea of Proof}
\protect\hypertarget{jl-lemma-idea-of-proof}{}
\begin{itemize}
\tightlist
\item
  Fix some vector \(x\) (wlog, let \(||x||=1\)) and use \(2\)-stability
  of Normal distribution \pause
\item
  (*) Let \(y = Mx\), so then \(y_i = \sum_{j=1}^k M_{ij} x_{i}\) \pause
\item
  (*) \(y\) is a Normal vector in \(\mathbb{R}^k\), and each \(y_i\) is
  \(\mathcal{N}(0,1)\) (variance because \(\sum x_i^2 = 1\)) \pause
\item
  (*) Let \(\alpha = \sum y_i^2\), so then \(\alpha \sim \chi^2 (k)\) \pause
\item
  (*) Thus
  \(\text{P}((1-\epsilon)^2 k \leq \alpha \leq (1+\epsilon)^2 k) \geq 1 - 2e^{O(1)\epsilon^2 k}\) \pause
\item
  (*) Picking \(k = \Omega\left(\frac{\log(1/\delta)}{\epsilon^2}\right)\)
  gets us the probability we want
\end{itemize}
\end{frame}

\section{Conclusion}

\frame{\sectionpage}

\begin{frame}{JL Lemma: Intuition and Application}
\protect\hypertarget{jl-lemma-intuition-and-application}{}
\begin{itemize}
\tightlist
\item
  Why does projecting to a random subspace work? A large enough random
  subspace means errors induced by ``bad vectors" (i.e.~those orthogonal
  to many rows in the matrix) have extremely low probability of ocurring \pause
\item
  Useful for tasks such as clustering/ML: things closer together/more
  similar in low dimension will be close in high dimension, so can
  reduce dimension and speed up clustering \pause
\item
  Coreset generation: Many hard geometric problems have fast approximate
  solutions via coreset technique, which generates a set \(S'\) from
  input \(S\) so that running an exact algorithm on \(S'\) generates a
  high accuracy approximation for that algorithm on \(S\). JL technique
  can be used in generating coresets \pause
\item
  Key advantage of JL is that it is \textit{oblivious} to data
\end{itemize}
\end{frame}

\begin{frame}{One more thing\ldots{}}
\protect\hypertarget{one-more-thing}{}
\begin{itemize}
\tightlist
\item
  JL Lemma extends to preserving vector distances in
  \textit{entire subspaces} of \(\mathbb{R}^n\)! \pause
\item
  Let \(E\) be a linear subspace of dimension \(d\) \pause
\item
  Can preserve distances between vectors in \(E\) with
  \(k = \Omega\left(\frac{d\log(1/\delta)}{\epsilon^2}\right)\) \pause
\item
  Works for all vectors in \(E\), even though there are infinitely many! \pause
\item
  Poof: consider partitioning the \(d\) dimensional unit ball into small
  hypercubes with small side length. Show that preserving lengths of
  vectors to these hypercubes is sufficient to preserve lengths of all
  vectors.
\end{itemize}
\end{frame}


\end{document}